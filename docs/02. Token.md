Token là các đơn vị cơ bản của văn bản sau khi được tách ra. Tùy vào chiến lược của mô hình, token có thể là từ nguyên, các phần của từ (subwords), ký tự, hoặc tổ hợp giữa từ và dấu câu.

Tokenization là bước đầu tiên trong quá trình training. LLM phân tích mối quan hệ ngữ nghĩa (semantic relationships) giữa các tokens, như tần suất chúng được sử dụng cùng nhau hoặc liệu chúng có thể sử dụng trong một bối cảnh tương tự hay không. Qua đó, mô hình nắm bắt được ngữ cảnh, cách sử dụng từ ngữ, cấu trúc câu và các yếu tố quan trọng khác trong ngôn ngữ.

Sau khi training, LLM sử dụng các mẫu đó và mối quan hệ giữa chúng để tạo ra chuỗi đầu ra dựa vào chuỗi đầu vào. 

## Chuyển văn bản thành tokens

Vocabulary (tập từ vựng) là tập hợp tất cả các token độc nhất mà mô hình được huấn luyện trên đó. Ví dụ, nếu mô hình được huấn luyện trên một lượng lớn văn bản, nó sẽ xây dựng một danh sách gồm hàng ngàn token khác nhau.

Khi có một văn bản đầu vào, mô hình sẽ chuyển đổi các từ và cụm từ thành các token có trong vocabulary của nó.

Ví dụ:

I heard a dog bark loudly at a cat

Đoạn text trên sẽ được tokenized như sau:

- I
- heard
- a
- dog
- bark
- loudly
- at
- a
- cat

Mỗi token này là một phần của vocabulary. Khi văn bản được chuyển thành dãy token, mô hình sẽ xử lý và hiểu nội dung dựa trên các token đó.

## Các phương pháp tokenization phổ biến

1. Word Tokenization (Tách từ):
- Cách hoạt động: Văn bản được chia thành các từ riêng lẻ dựa trên các dấu phân cách (như khoảng trắng, dấu câu).
- Ưu điểm:
    + Văn bản được chia thành số token ít hơn, giúp giảm tải tính toán khi xử lý.
    + Với cùng giới hạn token, kích thước đầu vào và đầu ra của mô hình có thể lớn hơn.
- Nhược điểm:
    + Tập từ vựng (vocabulary) có thể lớn lên rất nhanh, đòi hỏi bộ nhớ nhiều hơn.
    + Phương pháp này không linh hoạt với các từ không có trong từ điển, lỗi chính tả, hoặc cú pháp phức tạp, dẫn đến việc không nhận dạng được những từ mới.

2. Character Tokenization (Tách ký tự)
- Cách hoạt động: Văn bản được chia thành từng ký tự riêng lẻ.
- Ưu điểm:
    - Mô hình có thể xử lý hầu hết các ký tự, bao gồm cả từ ngữ lạ, lỗi chính tả hay biểu tượng đặc biệt.
    - Giúp giảm kích thước của từ vựng vì chỉ có một tập ký tự cố định.
- Nhược điểm:
    - Một văn bản sẽ bị chia thành nhiều token hơn, dẫn đến việc tính toán phức tạp và tăng thời gian xử lý.
    - Mối quan hệ ngữ nghĩa giữa các ký tự cần phải được học lại từ đầu, làm cho quá trình học của mô hình trở nên khó khăn hơn.
3. Subword Tokenization (Tách thành phần từ)
- Cách hoạt động: Văn bản được chia thành các phần nhỏ hơn của từ, có thể là gốc từ, tiền tố, hậu tố hoặc các tổ hợp ký tự.
- Ví dụ: GPT của OpenAI sử dụng phương pháp này thông qua kỹ thuật Byte-Pair Encoding (BPE), có thể test thử ở tool sau: [tokenizer](https://platform.openai.com/tokenizer)
- Ưu điểm:
    + Linh hoạt hơn so với word tokenization, giúp mô hình xử lý được từ mới, lỗi chính tả và cú pháp phức tạp.
    + Có thể giảm kích thước vocabulary so với word tokenization mà vẫn giữ được ý nghĩa của từ.
- Nhược điểm:
    + Văn bản ban đầu sẽ được tách thành nhiều token hơn so với word tokenization, từ đó tăng tải tính toán.
    + Với một giới hạn token cố định, kích thước đầu vào/đầu ra của mô hình có thể bị giới hạn hơn.

## LLMs sử dụng Token như thế nào

Sau khi LLM hoàn thành tokenization, nó sẽ gán ID cho mỗi token.

Ví dụ:

I heard a dog bark loudly at a cat

Giả sử mô hình sử dụng phương pháp Word Tokenization, nó sẽ gán token IDs như sau:

- I (1)
- heard (2)
- a (3)
- dog (4)
- bark (5)
- loudly (6)
- at (7)
- a (3)
- cat (8)

Bằng cách gán Token IDs, chuỗi trên có thể biểu diễn như sau:

[1, 2, 3, 4, 5, 6, 7, 3, 8]

Trong quá trình huấn luyện, mô hình đọc thêm các văn bản mới và gặp các token chưa xuất hiện trước đó, ví dụ:

- "meow" → sẽ được thêm vào từ vựng và gán ID là 9.
- "run" → sẽ được thêm vào từ vựng và gán ID là 10.

và sẽ được thêm vào tập từ vựng (vocabulary) của nó.

Semantic relationships giữa các tokens có thể được phân tích bằng cách sử dụng chuỗi Token Ids này. 

Mỗi token (với Id của nó) được gán một vector số nhiều chiều, gọi là embedding. Các embedding này chứa thông tin về ý nghĩa của token như mức độ token thường xuyên sử dụng cùng nhau, ngữ cảnh mà token xuất hiện. Ví dụ, các từ như "dog" và "cat" có thể có các embedding tương tự nhau vì chúng thường được sử dụng trong các ngữ cảnh liên quan đến động vật.

Khi mô hình xử lý một đoạn văn bản, nó lấy các embedding của các token đó, giúp hiểu mối liên hệ ngữ nghĩa giữa các token. Qua đó, mô hình có thể thực hiện các tác vụ như tìm kiếm tài liệu dựa trên ý nghĩa (semantic search) hoặc phân loại văn bản hoặc thêm một vector stores vào AI.

Trong quá trình tạo ra đầu ra, mô hình dự đoán một vector value cho token tiếp theo trong chuỗi. Sau đó, mô hình lựa chọn token tiếp theo từ tập từ vựng của nó dựa trên giá trị vector này. Trên thực tế, mô hình tính toán nhiều vector bằng cách sử dụng các yếu tố khác nhau từ các embedding của các token trước đó. Cuối cùng, mô hình đánh giá tất cả các token tiềm năng từ những vector này và chọn token có khả năng xuất hiện cao nhất để tiếp tục chuỗi.

Quá trình tạo đầu ra là một phép toán lặp lại. Mô hình thêm token dự đoán vào chuỗi hiện tại và sử dụng nó làm đầu vào cho vòng lặp tiếp theo, xây dựng đầu ra cuối cùng từng token một.

## Token limits

Giới hạn token là một hạn chế mà các mô hình ngôn ngữ lớn (LLMs) phải đối mặt, liên quan đến số lượng tối đa các token có thể được sử dụng làm đầu vào hoặc tạo ra làm đầu ra.  Giới hạn này ảnh hưởng đến khả năng mô hình xử lý văn bản dài, vì đầu vào và đầu ra phải chia sẻ một cửa sổ ngữ cảnh chung (context window) với tổng số token không vượt quá giới hạn này.

Giả sử mô hình có cửa sổ ngữ cảnh tối đa (maximum context window) 100 token. Điều này có nghĩa là tổng số token (đầu vào và đầu ra cộng lại) không thể vượt quá 100 token.

Ví dụ:
Câu nhập vào:
"I heard a dog bark loudly at a cat"

Phương pháp tokenization dựa trên từ (word-based tokenization):

Câu trên sẽ được chia thành 9 token (I, heard, a, dog, bark, loudly, at, a, cat).
Còn lại 91 token để sử dụng cho đầu ra.

Phương pháp tokenization dựa trên ký tự (character-based tokenization):

Mỗi ký tự, kể cả khoảng trắng, sẽ được tính là một token. Vì vậy, câu trên sẽ có 34 token.
Còn lại chỉ 66 token cho đầu ra.
