Mặc dù các LLM (ví dụ như GPT-3.5, GPT-4) được huấn luyện trên một lượng dữ liệu khổng lồ và có kiến thức tổng quát rất phong phú, nhưng chúng vẫn có thể không hiểu rõ những dữ liệu cụ thể, chuyên sâu của bạn nếu chỉ dựa vào dữ liệu huấn luyện ban đầu.

Thay vì phải huấn luyện lại LLM trên dữ liệu của riêng bạn, RAG cho phép bạn "kết nối" dữ liệu của mình với LLM thông qua một quá trình truy xuất thông tin (retrieval). Qua đó, khi người dùng đặt câu hỏi, hệ thống sẽ tìm ra những phần dữ liệu liên quan nhất và bổ sung vào prompt để LLM tạo ra phản hồi có tính ngữ cảnh và chính xác cao.

## Các Bước Cơ Bản Trong Quy Trình RAG

1. Xử lý nguồn dữ liệu (Source Data):
Nguồn dữ liệu có thể là file trên máy, kho lưu trữ đám mây, tài sản dữ liệu trên Azure Machine Learning, Git repository, cơ sở dữ liệu SQL, v.v.

2. Tách dữ liệu thành các khối nhỏ (Data Chunking):
Các tài liệu lớn (ví dụ: file Word, PDF) cần được chuyển đổi thành văn bản thuần (plain text) và sau đó được chia thành các đoạn (chunk) nhỏ hơn để dễ dàng xử lý và tìm kiếm.

3. Chuyển đổi văn bản thành vector (Embeddings):
- Sử dụng các mô hình embedding (như OpenAI's text-embedding-ada-002) để chuyển đổi từng khối văn bản thành các vector – đó là các dãy số đại diện cho nội dung và ý nghĩa ngữ cảnh của văn bản.
- Những vector này cho phép máy tính tính toán được độ tương đồng giữa các khối dữ liệu khác nhau.

4. Lưu trữ và lập chỉ mục (Indexing)
- Các vector được lưu trữ vào cơ sở dữ liệu vector hoặc dịch vụ tìm kiếm (ví dụ: Azure Cognitive Search hoặc Azure Cosmos DB với khả năng tìm kiếm vector).
- Đồng thời, lưu kèm theo các metadata liên quan đến dữ liệu gốc để có thể trích dẫn hoặc tham khảo khi cần.

5. Truy vấn và truy xuất thông tin

- Khi người dùng đưa ra một câu hỏi, câu hỏi đó cũng được chuyển đổi thành một vector thông qua cùng một mô hình embedding.

- Hệ thống so sánh vector của truy vấn với các vector đã lưu trữ, từ đó tìm ra các khối dữ liệu có độ tương đồng cao nhất (thường tính theo cosine similarity hoặc Euclidean distance).

6. Tạo prompt và sinh phản hồi:

- Các đoạn dữ liệu liên quan (ngữ cảnh) cùng với câu hỏi của người dùng sẽ được kết hợp thành một prompt gửi cho LLM (ví dụ: GPT-3.5 hoặc GPT-4).
- LLM sau đó sẽ sử dụng thông tin bổ sung này để tạo ra phản hồi có tính chất ngữ cảnh, chính xác và tự nhiên.

7. Lợi ích của metadata: Các metadata liên kết giữa dữ liệu gốc và embedding cho phép LLM tạo ra các phản hồi có thể trích dẫn nguồn dữ liệu, tăng tính minh bạch và độ tin cậy.
